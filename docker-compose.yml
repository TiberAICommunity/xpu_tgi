services:
  auth-service:
    build:
      context: ./services/auth
    container_name: tgi_auth
    environment:
      - VALID_TOKEN=${VALID_TOKEN}
    env_file:
      - .env
    networks:
      - tgi_net
    read_only: true
    security_opt:
      - no-new-privileges:true
    healthcheck:
      test:
        - CMD
        - curl
        - -f
        - http://localhost:3000/health
      interval: 10s
      timeout: 5s
      retries: 3
      start_period: 10s
    expose:
      - '3000'
    labels:
      - traefik.enable=true
      - traefik.http.services.auth.loadbalancer.server.port=3000
      - traefik.http.routers.auth.rule=PathPrefix(`/validate`)
    restart: unless-stopped
  proxy:
    image: traefik:v3.2.1
    container_name: tgi_proxy
    command:
      - --providers.docker=true
      - --providers.docker.exposedbydefault=false
      - --entrypoints.web.address=:${SERVICE_PORT}
      - --accesslog=true
      - --providers.file.directory=/etc/traefik/dynamic
    ports:
      - ${SERVICE_PORT}:${SERVICE_PORT}
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock:ro
      - ./traefik:/etc/traefik/dynamic:ro
    networks:
      - tgi_net
    depends_on:
      - auth-service
    env_file:
      - .env
    environment:
      - VALID_TOKEN=${VALID_TOKEN}
    healthcheck:
      test:
        - CMD
        - wget
        - --no-verbose
        - --tries=1
        - --spider
        - http://auth-service:3000/health
      interval: 10s
      timeout: 5s
      retries: 3
    restart: unless-stopped
  tgi:
    image: ghcr.io/huggingface/text-generation-inference:${TGI_VERSION}
    container_name: ${MODEL_NAME}
    restart: unless-stopped
    privileged: true
    cap_add:
      - sys_nice
    devices: ${GPU_DEVICES:-/dev/dri:/dev/dri}
    ipc: host
    shm_size: ${SHM_SIZE}
    expose:
      - '80'
    networks:
      - tgi_net
    environment:
      - VALID_TOKEN=${VALID_TOKEN}
      - MODEL_NAME=${MODEL_NAME}
      - MODEL_ID=${MODEL_ID}
      - MAX_CONCURRENT_REQUESTS=${MAX_CONCURRENT_REQUESTS}
      - MAX_BATCH_SIZE=${MAX_BATCH_SIZE}
      - MAX_TOTAL_TOKENS=${MAX_TOTAL_TOKENS}
      - MAX_INPUT_LENGTH=${MAX_INPUT_LENGTH}
      - MAX_WAITING_TOKENS=${MAX_WAITING_TOKENS}
      - HF_HOME=/root/.cache/huggingface
    command: |
      --model-id ${MODEL_ID} --dtype bfloat16 --max-concurrent-requests ${MAX_CONCURRENT_REQUESTS} --max-batch-size ${MAX_BATCH_SIZE} --max-total-tokens ${MAX_TOTAL_TOKENS} --max-input-length ${MAX_INPUT_LENGTH} --max-waiting-tokens ${MAX_WAITING_TOKENS} --cuda-graphs 0 --port 80 --json-output
    labels:
      - traefik.enable=true
      - traefik.http.routers.tgi.rule=PathPrefix(`/generate`)
      - traefik.http.routers.tgi.middlewares=chain-auth@file
      - traefik.http.services.tgi.loadbalancer.server.port=80
    env_file:
      - .env
    depends_on:
      auth-service:
        condition: service_healthy
      proxy:
        condition: service_healthy
    volumes:
      - ${HF_CACHE_DIR:-/tmp/no_cache}:/root/.cache/huggingface:rw
networks:
  tgi_net:
    name: ${MODEL_NAME}_network
    driver: bridge
